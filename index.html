<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: "Times New Roman", Times, serif;
      font-size: 16px
    }
    
    strong {
      font-family: "Times New Roman", Times, serif;
      font-size: 16px;
    }
    
    heading {
      font-family: "Times New Roman", Times, serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: "Times New Roman", Times, serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: "Times New Roman", Times, serif;
      font-size: 36px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      /* background-color: #ffffd0; */
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Elahe Vahdani</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Galdeano' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Elahe Vahdani</name>
              </p>
              <p>
		I am a computer vision researcher, specializing in video understanding. 
		I obtained my Ph.D. in Computer Science from The Graduate Center, The City University of New York, in Fall 2023, guided by Professor Yingli Tian. 
		My research primarily focused on 'Deep Learning-Based Human Action Understanding in Videos,' the topic of my Ph.D. thesis. 
		Prior to my doctoral studies, I obtained my Bachelor's degree in mathematics at Sharif University of Technology.
	      </p>
              <p align=center>
                <a href="mailto:evahdani@gradcenter.cuny.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=8UCruqIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/elahevahd">GitHub</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/elahe-vahdani/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
              <img src="images/Ellie_2024.jpg" width="250px">
            </td>
          </tr>
        </table>

<table width="100%" align="center" border="0" cellspacing="10" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
      <p>
        <strong>News:</strong>
        <br>
        <table border="1" cellpadding="10" cellspacing="0" width="100%">
          <tr>
            <th width="20%"><strong>Date</strong></th>
            <th width="80%"><strong>Event</strong></th>
          </tr>
          <tr>
            <td>July 2024</td>
            <td>I joined Microsoft as a Data Scientist, currently working on text-to-image retrieval for ads in the Microsoft Audience Network.</td>
          </tr>
          <tr>
            <td>December 2023</td>
            <td>I defended my Ph.D. thesis in Computer Science at The City University of New York, with a focus on Deep Learning-Based Human Action Understanding in Videos.</td>
          </tr>
          <tr>
            <td>Fall 2021</td>
            <td>I completed an internship at Dataminr as a Research Science Intern.</td>
          </tr>
          <tr>
            <td>Summer 2021</td>
            <td>I completed an internship at Expedia Group as a Data Science Intern.</td>
          </tr>
        </table>
        <br>
      </p>
    </td>
  </tr>
</table>

	
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research Projects</heading>
		<p> Below is a summary of my research projects encompassing various areas of computer vision. These include Action Detection in Untrimmed Videos, Sign Language Understanding, Cross-Modal Retrieval, and Multi-camera Vehicle Tracking and Re-identification. 
		  </span>
              </p>
            </td>
          </tr>
        </table>

	
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

         <tr onmouseout="CC_stop()" onmouseover="CC_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cc_image'><img src='images/potloc_fig.png' width="160px" height="160px"></div>
                <img src='images/potloc_GIF.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function CC_start() {
                  document.getElementById('cc_image').style.opacity = "1";
                }

                function CC_stop() {
                  document.getElementById('cc_image').style.opacity = "0";
                }
                CC_stop()
              </script>
            </td>
	

           <td valign="top" width="75%">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314224001255">
                <papertitle>POTLoc: Pseudo-label Oriented Transformer for point-supervised temporal Action Localization</papertitle>
              </a>
              <br>
       	      <strong>Elahe Vahdani</strong>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>
              <br>
              <em> CVIU </em>, 2024 
              <br>
              <a href="https://arxiv.org/pdf/2310.13585">PDF</a> 
              <p></p>
              <p>  A Pseudo-label Oriented Transformer for weakly-supervised Action Localization utilizing only point-level annotation.</p>		      
            </td>
          </tr>			

            <tr onmouseout="ASL_cont_stop()" onmouseover="ASL_cont_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='survey'><img src='images/intro-temporal-task_new.png' width="160px" height="160px"></div>
                <img src='images/task-relations.png' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function ASL_cont_start() {
                  document.getElementById('ASL_cont_image').style.opacity = "1";
                }

                function ASL_cont_stop() {
                  document.getElementById('ASL_cont_image').style.opacity = "0";
                }
                ASL_cont_stop()
              </script>
            </td>	
		    
           <td valign="top" width="75%">
              <a href="https://ieeexplore.ieee.org/document/9839464/">
                <papertitle>Deep Learning-based Action Detection in Untrimmed Videos: A Survey</papertitle>
              </a>
              <br>
       	      <strong>Elahe Vahdani</strong>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>
              <br>
              <em> TPAMI </em>, 2022 
              <br>
              <a href="https://arxiv.org/pdf/2110.00111.pdf">PDF</a> 
              <p></p>
              <p> An extensive overview of deep learning-based algorithms to tackle temporal action detection in untrimmed videos with different supervision levels.</p>		      
            </td>
          </tr>			

         <tr onmouseout="CC_stop()" onmouseover="CC_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cc_image'><img src='images/ccloss.PNG' width="160px" height="160px"></div>
                <img src='images/CMCL.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function CC_start() {
                  document.getElementById('cc_image').style.opacity = "1";
                }

                function CC_stop() {
                  document.getElementById('cc_image').style.opacity = "0";
                }
                CC_stop()
              </script>
            </td>
		    
	     <td valign="top" width="75%">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">
                <papertitle>Cross-Modal Center Loss for 3D Cross-Modal Retrieval</papertitle>
              </a>
              <br>
              <a href="https://longlong-jing.github.io/">Longlong Jing*</a>,
       	      <strong>Elahe Vahdani*</strong>,
              <a href="https://www.linkedin.com/in/jiaxing-tan-4797a163/">Jiaxing Tan</a>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>
              <br>
              <em> CVPR </em>, 2021 
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">PDF</a> 
              <p></p>
              <p>A novel cross-modal framework, designed to map representations from various modalities — such as images, mesh, and point-cloud — into a unified feature space.</p>
            </td>
          </tr>			
		
          

 	    	  <tr onmouseout="ASL_cont_stop()" onmouseover="ASL_cont_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ASL_cont_image'><img src='images/FE_figure.png' width="160px" height="160px"></div>
                <img src='images/FE_several.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function ASL_cont_start() {
                  document.getElementById('ASL_cont_image').style.opacity = "1";
                }

                function ASL_cont_stop() {
                  document.getElementById('ASL_cont_image').style.opacity = "0";
                }
                ASL_cont_stop()
              </script>
            </td>	
		    
		    
	      <td valign="top" width="75%">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413126&casa_token=Sw1ElmO9_SMAAAAA:ufV81LcMGcst_X-_qjRLbRFLPF-PCFWBhjRgmQMsGwwWw3lzmzFa6jtNW9BSg0XRtqG1zQipYg&tag=1">
                <papertitle>Recognizing American Sign Language Nonmanual Signal Grammar Errors in Continuous Videos</papertitle>
              </a>
              <br>
              <!-- <a href="http://timothybrooks.com/">Tim Brooks</a>, -->
       	      <strong>Elahe Vahdani</strong>,
	      <a href="https://longlong-jing.github.io/">Longlong Jing</a>,
              <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="https://huenerfauth.ist.rit.edu/">Matt Huenerfaut</a>
              <br>
              <em>ICPR </em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2005.00253.pdf">PDF</a> 
              <p></p>
              <p> We developed an educational tool that enables sign language students to automatically process their signing video assignments and receive immediate feedback on their fluency. This tool utilizes deep learning algorithms for the detection of grammatically important elements in continuous signing videos. </p>
            </td>
          </tr>

		
            <tr onmouseout="ASL_stop()" onmouseover="ASL_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ASL_image'><img src='images/sign-before.png' width="160px" height="160px"></div>
                <img src='images/ASL.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function ASL_start() {
                  document.getElementById('ASL_image').style.opacity = "1";
                }

                function ASL_stop() {
                  document.getElementById('ASL_image').style.opacity = "0";
                }
                ASL_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.worldscientific.com/doi/abs/10.1142/S2972335324500017">
                <papertitle>Multi-Modal Multi-Channel American Sign Language Recognition</papertitle>
              </a>
              <br>
              <!-- <a href="http://timothybrooks.com/">Tim Brooks</a>, -->
       	      <strong>Elahe Vahdani</strong>,
              <a href="https://longlong-jing.github.io/">Longlong Jing</a>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="https://huenerfauth.ist.rit.edu/">Matt Huenerfaut</a>
              <br>
              <em> IJAIR </em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/1906.02851.pdf">PDF</a> /
	      <a href="https://longlong-jing.github.io/ASL-100-RGBD/">Project Page</a> /
	      <a href="https://nyu.databrary.org/volume/1062">Dataset</a> /
              <p></p>
              <p>A multi-modal, multi-channel framework for the real-time recognition of American Sign Language (ASL) signs from RGB-D videos.</p>
            </td>
          </tr>

		
		
            <tr onmouseout="dataset_stop()" onmouseover="dataset_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='data_image'><img src='images/datachannels.PNG' width="160px" height="160px"></div>
                <img src='images/asldata.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function dataset_start() {
                  document.getElementById('data_image').style.opacity = "1";
                }

                function dataset_stop() {
                  document.getElementById('data_image').style.opacity = "0";
                }
                dataset_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.aclweb.org/anthology/2020.signlang-1.14.pdf">
                <papertitle>An Isolated-Signing RGBD Dataset of 100 American Sign Language Signs Produced by Fluent ASL Signers</papertitle>
              </a>
              <br> 
              <a href="https://www.linkedin.com/in/saadhassan1/">Saad Hassan</a>,
	      <a href="https://people.rit.edu/~lwb2627/">Larwan Berke</a>,
       	      <strong>Elahe Vahdani</strong>,
   	      <a href="https://longlong-jing.github.io/">Longlong Jing</a>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="https://huenerfauth.ist.rit.edu/">Matt Huenerfaut</a>
	      <br>
              <em>LREC </em>, 2020
              <br>
              <a href="https://www.aclweb.org/anthology/2020.signlang-1.14.pdf">PDF</a>/
	      <a href="https://longlong-jing.github.io/ASL-100-RGBD/">Project Page</a> /
	      <a href="https://nyu.databrary.org/volume/1062">Dataset</a> /
		    <p></p>
              <p>We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor.</p>
            </td>
          </tr>		
		
		
          <tr onmouseout="reid_stop()" onmouseover="reid_start()">
            <td width="25%">
              <div class="one">
                  <div class="two" id='reid_image'><img src='images/car-0.png' width="160px" height="160px"></div>
                <img src='images/vehicles.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function reid_start() {
                  document.getElementById('reid_image').style.opacity = "1";
                }

                function reid_stop() {
                  document.getElementById('reid_image').style.opacity = "0";
                }
                reid_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Chen_Multi-camera_Vehicle_Tracking_and_Re-identification_on_AI_City_Challenge_2019_CVPRW_2019_paper.pdf">
                <papertitle>Multi-camera Vehicle Tracking and Re-identification on AI City Challenge 2019</papertitle>
              </a>
              <br>
              <a href="https://cyccty.github.io/">Yucheng Chen</a>,
              <a href="https://longlong-jing.github.io/">Longlong Jing</a>,
              <strong>Elahe Vahdani</strong>,
	      <a href="https://www.linkedin.com/in/ling-zhang-cs/">Ling Zhang</a>,
              <a href="http://dianzi.nwpu.edu.cn/info/1269/5955.htm">Mingyi He</a>,
              <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>              
              <br>
              <em>CVPR AI City Workshop</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Chen_Multi-camera_Vehicle_Tracking_and_Re-identification_on_AI_City_Challenge_2019_CVPRW_2019_paper.pdf">PDF</a> /
	      <a href="">Slides</a> /
	      <a href="data/cvpr19_poster_AICITY.pdf">Poster</a>
              <p></p>
              <p> Our team's solutions for the image-based vehicle re-identification track and the multi-camera vehicle tracking track were featured in the AI City Challenge 2019. Our proposed framework significantly outperformed the current state-of-the-art vehicle ReID method, achieving a 16.3% improvement on the Veri dataset. </p>
            </td>
          </tr>

          <tr onmouseout="AA_stop()" onmouseover="AA_start()">
            <td width="25%">
              <div class="one">
                  <div class="two" id='AA_image'><img src='images/aligned_jobs.PNG' width="160px" height="160px"></div>
                <img src='images/scheduling.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function AA_start() {
                  document.getElementById('AA_image').style.opacity = "1";
                }

                function AA_stop() {
                  document.getElementById('AA_image').style.opacity = "0";
                }
                AA_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.researchgate.net/profile/Matthew_Johnson36/publication/318124139_Gathering_Information_in_Sensor_Networks_for_Synchronized_Freshness/links/5a6a7ccd0f7e9b1c12d185eb/Gathering-Information-in-Sensor-Networks-for-Synchronized-Freshness.pdf">
                <papertitle>Gathering Information in Sensor Networks for Synchronized Freshness</papertitle>
              </a>
              <br>
              <strong>Elahe Vahdani</strong>, Amotz Bar-Noy, Matthew P. Johnson, Tarek Abdelzaher         
              <br>
              <em>IEEE SECON</em>, 2017
              <br>
              <a href="https://www.researchgate.net/profile/Matthew_Johnson36/publication/318124139_Gathering_Information_in_Sensor_Networks_for_Synchronized_Freshness/links/5a6a7ccd0f7e9b1c12d185eb/Gathering-Information-in-Sensor-Networks-for-Synchronized-Freshness.pdf">PDF</a> 
              <p></p>
              <p> An approximation algorithm for the NP-hard optimization problem of scheduling a set of n given jobs, each with specific deadlines, using a minimum number of channels in a sensor network. </p>
            </td>
          </tr>

		
	<script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
