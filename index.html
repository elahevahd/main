<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: "Times New Roman", Times, serif;
      font-size: 16px
    }
    
    strong {
      font-family: "Times New Roman", Times, serif;
      font-size: 16px;
    }
    
    heading {
      font-family: "Times New Roman", Times, serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: "Times New Roman", Times, serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: "Times New Roman", Times, serif;
      font-size: 36px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      /* background-color: #ffffd0; */
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Elahe Vahdani</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Galdeano' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Elahe Vahdani</name>
              </p>
              <p>
                I'm a Ph.D. student in the <a href="http://media-lab.ccny.cuny.edu/wordpress/people/">Media Lab</a>, Dept. Computer Science at The City University of New York, advised by Professor <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Ying-Li Tian</a>. 
                My current research focuses on Video Analysis including Action Recognition and Temporal Action Detection. I received my Bachelor's degree from Sharif University of Technology.    
		    
	      </p>
              <p align=center>
                <a href="mailto:evahdani@gradcenter.cuny.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=8UCruqIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="data/Elahe_Resume.pdf">Curriculum Vitae</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/elahe-vahdani-345675a8/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
              <img src="images/eliimg.jpeg" width="250px">
            </td>
          </tr>
        </table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p>
                <strong>News:</strong>
                <br>
                <ul>
		  <li> Excited to join Dataminr as a Research Science Intern for Fall 2021.</li>
		  <br>
		  <li> Our paper "Cross-modal Center Loss" was accepted to CVPR 2021! </li>
		  <br
		  <li> Excited to join Expedia Group as a Data Science Intern for Summer 2021.</li>
		  <br>
		  <li> Received the M.Phil degree (Master of Philosophy) in Computer  Science, The City University of New York.</li>
		  <br>
		  <li> Advanced to Ph.D. candidacy.</li>  
<!-- 	          Title of the presentation: "Temporal Action Detection in Untrimmed Videos" -->
                </ul>
                <br>
              </p>
            </td>
          </tr>
        </table>

	
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
		<p>
                I'm mainly interested in computer vision, machine learning, and image processing. Currently, I'm working on Action Recognition, and Temporal Action Detection tasks, and am interested in designing algorithms with limited supervision, such as self-supervised and weakly-supervised learning. I have also worked on Facial Expression Analysis, Cross-Modality Bridging and Vehicle Re-identification projects.
            Prior to that, my research was focused on Approximation Algorithms for NP-Hard problems.</span>
              </p>
            </td>
          </tr>
        </table>

	
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <tr onmouseout="CC_stop()" onmouseover="CC_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cc_image'><img src='images/ccloss.PNG' width="160px" height="160px"></div>
                <img src='images/CMCL.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function CC_start() {
                  document.getElementById('cc_image').style.opacity = "1";
                }

                function CC_stop() {
                  document.getElementById('cc_image').style.opacity = "0";
                }
                CC_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2008.03561.pdf">
                <papertitle>Cross-modal Center Loss</papertitle>
              </a>
              <br>
              <a href="https://longlong-jing.github.io/">Longlong Jing*</a>,
       	      <strong>Elahe Vahdani*</strong>,
              <a href="https://www.linkedin.com/in/jiaxing-tan-4797a163/">Jiaxing Tan</a>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>
              <br>
              <em> CVPR 2021
              <br>
              <a href="https://arxiv.org/pdf/2008.03561.pdf">PDF</a> 
              <p></p>
              <p>We propose a novel cross-modal center loss to map the representations of different settings of modalities (e.g., images, mesh, point-cloud) into a common feature space.</p>
            </td>
          </tr>			
            <tr onmouseout="ASL_cont_stop()" onmouseover="ASL_cont_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ASL_cont_image'><img src='images/FE_figure.png' width="160px" height="160px"></div>
                <img src='images/FE_several.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function ASL_cont_start() {
                  document.getElementById('ASL_cont_image').style.opacity = "1";
                }

                function ASL_cont_stop() {
                  document.getElementById('ASL_cont_image').style.opacity = "0";
                }
                ASL_cont_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2005.00253.pdf">
                <papertitle>Recognizing American Sign Language Nonmanual Signal Grammar Errors in Continuous Videos</papertitle>
              </a>
              <br>
              <!-- <a href="http://timothybrooks.com/">Tim Brooks</a>, -->
       	      <strong>Elahe Vahdani*</strong>,
	      <a href="https://longlong-jing.github.io/">Longlong Jing*</a>,
              <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="https://huenerfauth.ist.rit.edu/">Matt Huenerfaut</a>
              <br>
              <em>ICPR </em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2005.00253.pdf">PDF</a> 
              <p></p>
              <p> We designed an educational tool for sign language students to automatically process their signing video assignments and send them an immediate feedback regarding the fluency of their signing. The framework is based on deep-learning algorithms for temporal detection of grammatically important elements from continuous signing videos, and checking their correspondence in multiple modalities such as facial expression, head movements and hand gestures.</p>
            </td>
          </tr>

		
            <tr onmouseout="ASL_stop()" onmouseover="ASL_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ASL_image'><img src='images/sign-before.png' width="160px" height="160px"></div>
                <img src='images/ASL.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function ASL_start() {
                  document.getElementById('ASL_image').style.opacity = "1";
                }

                function ASL_stop() {
                  document.getElementById('ASL_image').style.opacity = "0";
                }
                ASL_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/1906.02851.pdf">
                <papertitle>Recognizing American Sign Language Manual Signs from RGB-D Videos</papertitle>
              </a>
              <br>
              <!-- <a href="http://timothybrooks.com/">Tim Brooks</a>, -->
              <a href="https://longlong-jing.github.io/">Longlong Jing*</a>,
       	      <strong>Elahe Vahdani*</strong>,
              <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="https://huenerfauth.ist.rit.edu/">Matt Huenerfaut</a>
              <br>
              <em>Under Review </em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1906.02851.pdf">PDF</a> /
	      <a href="https://longlong-jing.github.io/ASL-100-RGBD/">Project Page</a> /
	      <a href="https://nyu.databrary.org/volume/1062">Dataset</a> /
              <p></p>
              <p>We propose a 3D ConvNet based multi-stream framework to recognize American Sign Language (ASL) manual signs in real-time from RGB-D videos.</p>
            </td>
          </tr>

		
		
            <tr onmouseout="dataset_stop()" onmouseover="dataset_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='data_image'><img src='images/datachannels.PNG' width="160px" height="160px"></div>
                <img src='images/asldata.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function dataset_start() {
                  document.getElementById('data_image').style.opacity = "1";
                }

                function dataset_stop() {
                  document.getElementById('data_image').style.opacity = "0";
                }
                dataset_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.aclweb.org/anthology/2020.signlang-1.14.pdf">
                <papertitle>An Isolated-Signing RGBD Dataset of 100 American Sign Language Signs Produced by Fluent ASL Signers</papertitle>
              </a>
              <br> 
              <a href="https://www.linkedin.com/in/saadhassan1/">Saad Hassan</a>,
	      <a href="https://people.rit.edu/~lwb2627/">Larwan Berke</a>,
       	      <strong>Elahe Vahdani</strong>,
   	      <a href="https://longlong-jing.github.io/">Longlong Jing</a>,
	      <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="https://huenerfauth.ist.rit.edu/">Matt Huenerfaut</a>
	      <br>
              <em>LREC </em>, 2020
              <br>
              <a href="https://www.aclweb.org/anthology/2020.signlang-1.14.pdf">PDF</a>/
	      <a href="https://longlong-jing.github.io/ASL-100-RGBD/">Project Page</a> /
	      <a href="https://nyu.databrary.org/volume/1062">Dataset</a> /
		    <p></p>
              <p>We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor.</p>
            </td>
          </tr>		
		
		
          <tr onmouseout="reid_stop()" onmouseover="reid_start()">
            <td width="25%">
              <div class="one">
                  <div class="two" id='reid_image'><img src='images/car-0.png' width="160px" height="160px"></div>
                <img src='images/vehicles.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function reid_start() {
                  document.getElementById('reid_image').style.opacity = "1";
                }

                function reid_stop() {
                  document.getElementById('reid_image').style.opacity = "0";
                }
                reid_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Chen_Multi-camera_Vehicle_Tracking_and_Re-identification_on_AI_City_Challenge_2019_CVPRW_2019_paper.pdf">
                <papertitle>Multi-camera Vehicle Tracking and Re-identification on AI City Challenge 2019</papertitle>
              </a>
              <br>
              <a href="https://cyccty.github.io/">Yucheng Chen</a>,
              <a href="https://longlong-jing.github.io/">Longlong Jing</a>,
              <strong>Elahe Vahdani</strong>,
	      <a href="https://www.linkedin.com/in/ling-zhang-cs/">Ling Zhang</a>,
              <a href="http://dianzi.nwpu.edu.cn/info/1269/5955.htm">Mingyi He</a>,
              <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>              
              <br>
              <em>CVPR AI City Workshop</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Chen_Multi-camera_Vehicle_Tracking_and_Re-identification_on_AI_City_Challenge_2019_CVPRW_2019_paper.pdf">PDF</a> /
	      <a href="">Slides</a> /
	      <a href="data/cvpr19_poster_AICITY.pdf">Poster</a>
              <p></p>
              <p> Our solutions to the image-based vehicle re-identification track and multi-camera vehicle tracking track on AI City Challenge 2019 (AIC2019).  Our proposed
                  framework outperforms the current state-of-the-art vehicle ReID method by 16.3% on Veri dataset. </p>
            </td>
          </tr>

          <tr onmouseout="AA_stop()" onmouseover="AA_start()">
            <td width="25%">
              <div class="one">
                  <div class="two" id='AA_image'><img src='images/aligned_jobs.PNG' width="160px" height="160px"></div>
                <img src='images/scheduling.gif' width="160px" height="160px">
              </div>
              <script type="text/javascript">
                function AA_start() {
                  document.getElementById('AA_image').style.opacity = "1";
                }

                function AA_stop() {
                  document.getElementById('AA_image').style.opacity = "0";
                }
                AA_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.researchgate.net/profile/Matthew_Johnson36/publication/318124139_Gathering_Information_in_Sensor_Networks_for_Synchronized_Freshness/links/5a6a7ccd0f7e9b1c12d185eb/Gathering-Information-in-Sensor-Networks-for-Synchronized-Freshness.pdf">
                <papertitle>Gathering Information in Sensor Networks for Synchronized Freshness</papertitle>
              </a>
              <br>
              <strong>Elahe Vahdani</strong>, Amotz Bar-Noy, Matthew P. Johnson, Tarek Abdelzaher         
              <br>
              <em>IEEE SECON</em>, 2017
              <br>
              <a href="https://www.researchgate.net/profile/Matthew_Johnson36/publication/318124139_Gathering_Information_in_Sensor_Networks_for_Synchronized_Freshness/links/5a6a7ccd0f7e9b1c12d185eb/Gathering-Information-in-Sensor-Networks-for-Synchronized-Freshness.pdf">PDF</a> 
              <p></p>
              <p> Designed an approximation algorithm for the optimization problem of scheduling a set of n given jobs with their specific deadlines via a minimum number of channels in a sensor network. We proved the problem is NP-hard and provided a O(log n)- approximation algorithm. </p>
            </td>
          </tr>

		
	<script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
